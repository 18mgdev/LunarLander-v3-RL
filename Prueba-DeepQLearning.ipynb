{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install swig -q\n",
        "%pip install gymnasium[box2d] -q\n",
        "%pip install loky -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwQQt1IX95iG",
        "outputId": "1468fe25-f66b-405b-88bd-394618128c37"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "soAikAv99hqx"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('LunarLander-v3')\n",
        "num_actions = env.action_space.n"
      ],
      "metadata": {
        "id": "494dTixw9t3R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(tf.keras.Model):\n",
        "    \"\"\"Perceptron multicapa de 2 capas de 32 y una de salida.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.dense1 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
        "        self.dense2 = tf.keras.layers.Dense(32, activation=\"relu\")\n",
        "        self.dense3 = tf.keras.layers.Dense(num_actions, dtype=tf.float32)\n",
        "        # No activation definida para la capa de salida\n",
        "\n",
        "    def call(self, x):\n",
        "        \"\"\"Construcción de las capas.\"\"\"\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "\n",
        "# Red principal\n",
        "main_nn = DQN()\n",
        "# Red objetivo\n",
        "target_nn = DQN()\n",
        "\n",
        "# Optimizador Adam\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "# Función de pérdida (MSE)\n",
        "mse = tf.keras.losses.MeanSquaredError()\n"
      ],
      "metadata": {
        "id": "xrgAGtV3-ajU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer(object):\n",
        "    \"\"\"Experience replay buffer that samples uniformly.\"\"\"\n",
        "\n",
        "    def __init__(self, size):\n",
        "        self.buffer = deque(maxlen=size)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def sample(self, num_samples):\n",
        "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "\n",
        "        idx = np.random.choice(len(self.buffer), num_samples)\n",
        "        for i in idx:\n",
        "            elem = self.buffer[i]\n",
        "            state, action, reward, next_state, done = elem\n",
        "\n",
        "            states.append(np.array(state, copy=False))\n",
        "            actions.append(np.array(action, copy=False))\n",
        "            rewards.append(reward)\n",
        "            next_states.append(np.array(next_state, copy=False))\n",
        "            dones.append(done)\n",
        "\n",
        "        states = np.array(states)\n",
        "        actions = np.array(actions)\n",
        "        rewards = np.array(rewards, dtype=np.float32)\n",
        "        next_states = np.array(next_states)\n",
        "        dones = np.array(dones, dtype=np.float32)\n",
        "\n",
        "        return states, actions, rewards, next_states, dones\n"
      ],
      "metadata": {
        "id": "ymW_2xaq_iQz"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_epsilon_greedy_action(state, epsilon):\n",
        "    \"\"\"Acción aleatoria con probabilidad menor que epsilon, en otro caso la mejor.\"\"\"\n",
        "    result = tf.random.uniform((1,))\n",
        "    if result < epsilon:\n",
        "        # Elegimos una acción aleatoria\n",
        "        return env.action_space.sample()\n",
        "    else:\n",
        "        # Elección de acción Greedy\n",
        "        return tf.argmax(main_nn(state)[0]).numpy()\n",
        "\n",
        "@tf.function\n",
        "def train_step(states, actions, rewards, next_states, dones):\n",
        "    \"\"\"Configuración de cada iteración de entrenamiento.\"\"\"\n",
        "    # Cálculo de los objetivos (segunda red)\n",
        "    next_qs = target_nn(next_states)\n",
        "    max_next_qs = tf.reduce_max(next_qs, axis=-1)\n",
        "    target = rewards + (1. - dones) * discount * max_next_qs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        qs = main_nn(states)\n",
        "        action_masks = tf.one_hot(actions, num_actions)\n",
        "        masked_qs = tf.reduce_sum(action_masks * qs, axis=-1)\n",
        "        loss = mse(target, masked_qs)\n",
        "\n",
        "    grads = tape.gradient(loss, main_nn.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, main_nn.trainable_variables))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "EVBSKlq9_yLI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "state = env.reset()\n",
        "print(\"state type:\", type(state))\n",
        "print(\"state:\", state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBle-1ZjBwYv",
        "outputId": "86a4f335-c991-4cd2-da32-3c6e3bd9a0e9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "state type: <class 'tuple'>\n",
            "state: (array([-0.00364151,  1.4190147 , -0.3688649 ,  0.35975143,  0.00422643,\n",
            "        0.08355333,  0.        ,  0.        ], dtype=float32), {})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparámetros\n",
        "num_episodes = 1000\n",
        "epsilon = 1.0\n",
        "batch_size = 32\n",
        "discount = 0.99\n",
        "buffer = ReplayBuffer(100000)\n",
        "cur_frame = 0\n",
        "\n",
        "# Comienzo del entrenamiento. Jugamos una vez y entrenamos con un batch.\n",
        "last_100_ep_rewards = []\n",
        "\n",
        "for episode in range(num_episodes + 1):\n",
        "    state,_ = env.reset()  # Reseteo del ecosistema\n",
        "    ep_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        state_in = tf.expand_dims(np.asarray(state, dtype=np.float32), axis=0)\n",
        "        action = select_epsilon_greedy_action(state_in, epsilon)\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        ep_reward += reward\n",
        "\n",
        "        # Guardamos la experiencia.\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "        state = next_state\n",
        "        cur_frame += 1\n",
        "\n",
        "        # Copiamos los pesos de main_nn a target_nn cada 2000 frames.\n",
        "        if cur_frame % 2000 == 0:\n",
        "            target_nn.set_weights(main_nn.get_weights())\n",
        "\n",
        "        # Entrenamiento de la red neuronal.\n",
        "        if len(buffer) >= batch_size:\n",
        "            states, actions, rewards, next_states, dones = buffer.sample(batch_size)\n",
        "            loss = train_step(states, actions, rewards, next_states, dones)\n",
        "\n",
        "    # Actualización de epsilon mientras no se llegue a 950 episodios.\n",
        "    if episode < 950:\n",
        "        epsilon -= 0.001\n",
        "\n",
        "    # Mantenimiento de la lista de recompensas de los últimos 100 episodios.\n",
        "    if len(last_100_ep_rewards) == 100:\n",
        "        last_100_ep_rewards = last_100_ep_rewards[1:]\n",
        "    last_100_ep_rewards.append(ep_reward)\n",
        "\n",
        "    # Impresión del progreso cada 50 episodios.\n",
        "    if episode % 50 == 0:\n",
        "        print(f'Episode {episode}/{num_episodes}. Epsilon: {epsilon:.3f}. '\n",
        "              f'Reward in last 100 episodes: {np.mean(last_100_ep_rewards):.3f}')\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jucLCQBAENx",
        "outputId": "46962e03-b4bb-471c-dacb-d182e2609c24"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0/1000. Epsilon: 0.999. Reward in last 100 episodes: -109.824\n",
            "Episode 50/1000. Epsilon: 0.949. Reward in last 100 episodes: -181.054\n",
            "Episode 100/1000. Epsilon: 0.899. Reward in last 100 episodes: -174.349\n",
            "Episode 150/1000. Epsilon: 0.849. Reward in last 100 episodes: -166.462\n",
            "Episode 200/1000. Epsilon: 0.799. Reward in last 100 episodes: -159.868\n",
            "Episode 250/1000. Epsilon: 0.749. Reward in last 100 episodes: -140.349\n",
            "Episode 300/1000. Epsilon: 0.699. Reward in last 100 episodes: -126.237\n",
            "Episode 350/1000. Epsilon: 0.649. Reward in last 100 episodes: -111.820\n",
            "Episode 400/1000. Epsilon: 0.599. Reward in last 100 episodes: -75.713\n",
            "Episode 450/1000. Epsilon: 0.549. Reward in last 100 episodes: -56.561\n",
            "Episode 500/1000. Epsilon: 0.499. Reward in last 100 episodes: -47.810\n",
            "Episode 550/1000. Epsilon: 0.449. Reward in last 100 episodes: -47.941\n",
            "Episode 600/1000. Epsilon: 0.399. Reward in last 100 episodes: -69.999\n",
            "Episode 650/1000. Epsilon: 0.349. Reward in last 100 episodes: -53.414\n",
            "Episode 700/1000. Epsilon: 0.299. Reward in last 100 episodes: -14.122\n",
            "Episode 750/1000. Epsilon: 0.249. Reward in last 100 episodes: 23.443\n",
            "Episode 800/1000. Epsilon: 0.199. Reward in last 100 episodes: 69.105\n",
            "Episode 850/1000. Epsilon: 0.149. Reward in last 100 episodes: 94.886\n",
            "Episode 900/1000. Epsilon: 0.099. Reward in last 100 episodes: 96.452\n",
            "Episode 950/1000. Epsilon: 0.050. Reward in last 100 episodes: 75.231\n",
            "Episode 1000/1000. Epsilon: 0.050. Reward in last 100 episodes: 64.736\n"
          ]
        }
      ]
    }
  ]
}